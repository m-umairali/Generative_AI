{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Deep Learning\n",
    "\n",
    "\n",
    "*What I cannot create, I do not understand.*\n",
    "\n",
    "-Richard Feynman\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**This book is divided into three parts.**\n",
    "\n",
    "\n",
    "\n",
    "**Part I** is a general introduction to generative modeling and deep learning, where we explore the core concepts that underpin all of the techniques in later parts of the book:\n",
    "\n",
    "**Part II** walks through in building generative models, with practical examples.\n",
    "\n",
    "**Part III** will be consist of state-of-the-art models for image generation, writing, composing music, and model-based reinforcement learning\n",
    "\n",
    "## What Is Generative Modeling?\n",
    "*Generative modeling is a branch of machine learning that involves training a model to produce new data that is similar to a given dataset.*\n",
    "\n",
    "A generative model must also be probabilistic rather than deterministic, because we want to be able to sample many different variations of the output, rather than get the same output every time.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Versus Discriminative Modeling\n",
    "Generative modeling and discriminative modeling are two approaches in machine learning. Discriminative modeling focuses on predicting the label or class of a given input, while generative modeling aims to generate new examples similar to the ones in the training data.\n",
    "\n",
    "To illustrate the difference, let's consider a dataset of paintings by various artists, including Van Gogh. In discriminative modeling, we can train a model to predict whether a painting is by Van Gogh or not. The model learns to identify specific colors, shapes, and textures associated with Van Gogh's style. It assigns higher probabilities to paintings with these features, enabling it to make predictions. Van Gogh paintings are labeled as 1, and non-Van Gogh paintings are labeled as 0. \n",
    "\n",
    "In contrast, generative modeling doesn't require labeled data. Its goal is to generate new images that resemble the ones in the training data. Instead of predicting the label of a given image, a generative model focuses on creating entirely new images that are similar in style, color, and structure to the original paintings.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Representation learning\n",
    "To illustrate this concept, imagine describing your appearance to someone who is trying to find you in a crowd. Instead of listing the color of each pixel in a photo of yourself, you would assume that the other person has a general idea of what an average human looks like and provide additional features that describe groups of pixels, such as having blond hair or wearing glasses. By using a few of these statements, the other person can generate an image of you in their mind, even if they have never seen you before.\n",
    "\n",
    "Representation learning works similarly by describing each observation in a lower-dimensional latent space instead of directly modeling the high-dimensional sample space. In other words, each point in the latent space represents a high-dimensional observation\n",
    "\n",
    "The benefit of utilizing a latent space is that operations can be performed on the representation vectors within this space, affecting high-level properties of the image\n",
    "\n",
    "Machine learning, particularly deep learning, enables machines to discover and learn these complex relationships between the high-dimensional data and its meaningful representations without explicit human guidance. By encoding the training dataset into a latent space, it becomes possible to sample from it and decode the points back to the original domain, which is a common approach in various generative modeling techniques. These techniques aim to transform the nonlinear manifold of the data (e.g., pixel space) into a simpler latent space that can be sampled from, ensuring that any point in the latent space represents a well-formed image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Probability Theories for Gen AI\n",
    "generative modeling is closely connected to statistical modeling of probability distributions. Therefore, it makes sense to learn some core probabilistic and statistical concepts:\n",
    "\n",
    "### Sample Space:\n",
    "- The sample space, also known as the outcome space or possibility space, refers to the set of all possible values or outcomes that an observation, denoted as `'x'` can take. It represents the entire range of potential results or events that can occur in a given context or experiment. For example, suppose we are rolling a fair six-sided die. The sample space for this experiment would consist of the numbers `{1, 2, 3, 4, 5, 6}` as these are all the possible outcomes or values that the observation (the rolled number) can take.\n",
    "\n",
    "### Probability density function (PDF):\n",
    "- A probability density function (PDF) is a function, denoted as p(x), that assigns a probability value between 0 and 1 to each point x in the sample space. It describes the likelihood of a random variable taking on a specific value within the sample space. \n",
    "In the context of generative modeling, there is a true density function, pdata(x), that represents the underlying distribution that generated the observed dataset. However, when estimating this true density function, we typically use a model density function, pmodel(x), which is an approximation. There can be infinitely many possible model density functions that can be used to estimate the true distribution.\n",
    "The goal of generative modeling is to learn a model density function that closely matches the true density function, pdata(x), in order to generate new samples that resemble the data distribution. By estimating the model density function, we can sample from it to generate new data points or perform various probabilistic calculations and analysis.\n",
    "\n",
    "### Parametric modeling\n",
    "- Parametric modeling is an approach where we structure our method of finding a suitable model density function, pmodel(x), by using a family of density functions that can be described with a finite number of parameters. These parameters are denoted as Î¸. In parametric modeling, we make an assumption about the form or shape of the model density function, and this assumption is determined by the choice of parameters. By varying the values of these parameters, we can generate different density functions within the chosen family.\n",
    "\n",
    "### Likelihood:\n",
    "- Likelihood measures the plausibility of a set of parameters given the observed data. It quantifies how well the parameters can explain the data. The likelihood is calculated by evaluating the density function at the data points. The log-likelihood is often used for computational convenience. The focus of parametric modeling is to find the parameter set that maximizes the likelihood, leading to the best-fitting model to the observed data.\n",
    "\n",
    "### maximum likelihood estimation\n",
    "- Maximum likelihood estimation is a technique used to estimate the parameter values that maximize the likelihood of observing the given data. It involves finding the parameter set that best explains the data according to a given density function. For generative models, MLE corresponds to finding the optimal weights of the model that maximize the likelihood or minimize the negative log-likelihood. However, calculating the density function directly can be challenging for high-dimensional problems, and different generative models employ different strategies to overcome this issue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Model Approaches:\n",
    "**- Explicit modeling:** In this approach, the model explicitly defines a mathematical form for the density function, which describes how likely different data points are. The model is designed in such a way that the density function can be calculated or estimated directly. It's like having a formula or equation that tells you the probability of each data point. \n",
    "\n",
    "**- Approximating the density function:** Instead of explicitly modeling the density function, this approach focuses on finding a simpler and tractable approximation of it. The model doesn't give you the exact formula, but it provides an approximation that is close enough to the true distribution. It's like having a rough estimate or a simplified version of the probability distribution. Variational Autoencoders (VAEs) and Normalizing Flows are examples of models that approximate the density function with simpler distributions.\n",
    "\n",
    "**- Implicit modeling through a stochastic process:** In this approach, the model doesn't explicitly define the density function or its approximation. Instead, it learns to generate data by simulating a random process. It's like having a set of rules or procedures that generate new data samples. Generative Adversarial Networks (GANs) are an example of models that work this way. They consist of a generator network that generates samples and a discriminator network that learns to differentiate between real and generated samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*clone this repo to get started*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "1. What is the difference between generative and discriminative modeling in machine learning?\n",
    "2. How do generative models represent data in a lower-dimensional space?\n",
    "3. What are the different approaches in generative modeling?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><b>Click Here To See The Answers</b></summary>\n",
    "  \n",
    " \n",
    "1. Generative modeling focuses on generating new data that resembles the training data, while discriminative modeling aims to predict the label or class of a given input.\n",
    "\n",
    "2. Generative models use representation learning to describe each observation in a simpler latent space instead of directly modeling the high-dimensional sample space. This allows operations and transformations to be performed on the representation vectors, affecting high-level properties of the data.\n",
    "\n",
    "3. Generative models can either explicitly model the density function, approximating it or implicitly model it through a stochastic process. Explicit modeling involves either directly optimizing the density function or optimizing an approximation of it. Implicit modeling focuses on directly generating data through a stochastic process, without explicitly estimating the probability density.\n",
    "\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
